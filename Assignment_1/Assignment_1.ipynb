{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302d2d0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.391791800Z",
     "start_time": "2024-01-30T17:20:39.376226400Z"
    }
   },
   "outputs": [],
   "source": [
    "#downloading the packages\n",
    "# uncomment all the lines to install the packages\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# open the nltk downloader\n",
    "# note that the downloader might be minimized in your toolbar\n",
    "# the downloader is a modal window, so the Jupyter notebook will wait for you to do something with it\n",
    "#nltk.download()\n",
    "\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83b13f64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.425867200Z",
     "start_time": "2024-01-30T17:20:39.382736400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Tokenize the texts in the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1107e1a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.426839400Z",
     "start_time": "2024-01-30T17:20:39.399737900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text written to: E:\\MS_Course_Notes\\COMP_293C\\Assignments\\Assignment_1\\q1_tokenized_output.txt\n"
     ]
    }
   ],
   "source": [
    "#using nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "# Open the input file for reading\n",
    "# change the path of the input file according to your path location\n",
    "with open(\"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\input_data.txt\", 'r') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "tokenized_text = word_tokenize(input_text)\n",
    "\n",
    "# Specify the path for the output file to save the tokenized text\n",
    "output_file_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q1_tokenized_output.txt\"\n",
    "\n",
    "# Write the tokenized text to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(str(tokenized_text))\n",
    "\n",
    "print(\"Tokenized text written to:\", output_file_path)\n",
    "#print the tokenized text\n",
    "#print(tokenized_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a288f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.432839300Z",
     "start_time": "2024-01-30T17:20:39.412737200Z"
    }
   },
   "outputs": [],
   "source": [
    "#2. Count word frequencies in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "670b0943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.443839500Z",
     "start_time": "2024-01-30T17:20:39.431840500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count word frequencies written to: E:\\MS_Course_Notes\\COMP_293C\\Assignments\\Assignment_1\\q2_frequency_output.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": "FreqDist({',': 74, 'the': 70, 'to': 55, ':': 52, '.': 44, 'of': 41, 'your': 40, 'for': 35, 'are': 26, 'and': 23, ...})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(tokenized_text)\n",
    "\n",
    "# Convert the frequency distribution to a dictionary\n",
    "freq_dict = dict(freq_dist)\n",
    "\n",
    "\n",
    "# Specify the path for the output file to save the tokenized text\n",
    "output_file_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q2_frequency_output.txt\"\n",
    "\n",
    "# Write the tokenized text to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(str(freq_dict))\n",
    "\n",
    "print(\"Count word frequencies written to:\", output_file_path)\n",
    "FreqDist(tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44dac5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.504352600Z",
     "start_time": "2024-01-30T17:20:39.445841200Z"
    }
   },
   "outputs": [],
   "source": [
    "#3. Perform part-of-speech (POS) tagging on the tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a01523c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.513074100Z",
     "start_time": "2024-01-30T17:20:39.461840100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform part-of-speech (POS) tagging on the tokenized words written to: E:\\MS_Course_Notes\\COMP_293C\\Assignments\\Assignment_1\\q3_pos_output.txt\n"
     ]
    }
   ],
   "source": [
    "pos_tag_text = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "# Specify the path for the output file to save the tokenized text\n",
    "output_file_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q3_pos_output.txt\"\n",
    "\n",
    "# Write the tokenized text to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(str(pos_tag_text))\n",
    "\n",
    "print(\"Perform part-of-speech (POS) tagging on the tokenized words written to:\", output_file_path)\n",
    "\n",
    "#print(pos_tag_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07db0b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.524087600Z",
     "start_time": "2024-01-30T17:20:39.509354500Z"
    }
   },
   "outputs": [],
   "source": [
    "#4. Perform named entity recognition (NER) on the texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d31830c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.726406300Z",
     "start_time": "2024-01-30T17:20:39.526087100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform named entity recognition (NER) on the texts written to: E:\\MS_Course_Notes\\COMP_293C\\Assignments\\Assignment_1\\q4_pos_output.txt\n"
     ]
    }
   ],
   "source": [
    "# Use NLTK's NER chunker\n",
    "ner_tag_texts = nltk.chunk.ne_chunk(pos_tag_text)\n",
    "\n",
    "# 'ner_tag_texts' now contains a tree structure with named entities recognized\n",
    "# Extract named entities as a list:\n",
    "named_entities = []\n",
    "for subtree in ner_tag_texts:\n",
    "    if isinstance(subtree, nltk.Tree):\n",
    "        entity = \" \".join([word for word, tag in subtree.leaves()])\n",
    "        named_entities.append((entity, subtree.label()))\n",
    "        \n",
    "# Specify the path for the output file to save the tokenized text\n",
    "output_file_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q4_pos_output.txt\"\n",
    "\n",
    "# Write the tokenized text to the output file\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(str(named_entities))\n",
    "\n",
    "print(\"Perform named entity recognition (NER) on the texts written to:\", output_file_path)        \n",
    "#print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe0f6df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:39.741509400Z",
     "start_time": "2024-01-30T17:20:39.726406300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Displaying the most frequent 10 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b92c31ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:40.143736700Z",
     "start_time": "2024-01-30T17:20:39.741509400Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# NLP imports\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m displacy\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# general numerical and visualization imports\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\__init__.py:13\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# These are imported as part of the API\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mthinc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m util\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabout\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mattributeruler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AttributeRuler\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdep_parser\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DependencyParser\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01medit_tree_lemmatizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EditTreeLemmatizer\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m util\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Errors\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Language\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmatcher\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Matcher\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscorer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Scorer\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\language.py:43\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlang\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenizer_exceptions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlookups\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_lookups\n\u001B[1;32m---> 43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipe_analysis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mschemas\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     45\u001B[0m     ConfigSchema,\n\u001B[0;32m     46\u001B[0m     ConfigSchemaInit,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     49\u001B[0m     validate_init_settings,\n\u001B[0;32m     50\u001B[0m )\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscorer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Scorer\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\pipe_analysis.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwasabi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m msg\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Errors\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokens\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Doc, Span, Token\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dot_to_dict\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_serialize\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DocBin\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdoc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Doc\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmorphanalysis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MorphAnalysis\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Errors\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SimpleFrozenList, ensure_path\n\u001B[1;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvocab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Vocab\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dict_proxies\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SpanGroups\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdoc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DOCBIN_ALL_ATTRS \u001B[38;5;28;01mas\u001B[39;00m ALL_ATTRS\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\vocab.pyx:1\u001B[0m, in \u001B[0;36minit spacy.vocab\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001B[0m, in \u001B[0;36minit spacy.tokens.doc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\schemas.py:287\u001B[0m\n\u001B[0;32m    281\u001B[0m UnderscoreValue \u001B[38;5;241m=\u001B[39m Union[\n\u001B[0;32m    282\u001B[0m     TokenPatternString, TokenPatternNumber, \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mbool\u001B[39m\n\u001B[0;32m    283\u001B[0m ]\n\u001B[0;32m    284\u001B[0m IobValue \u001B[38;5;241m=\u001B[39m Literal[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTokenPattern\u001B[39;00m(BaseModel):\n\u001B[0;32m    288\u001B[0m     orth: Optional[StringValue] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    289\u001B[0m     text: Optional[StringValue] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\main.py:299\u001B[0m, in \u001B[0;36mpydantic.main.ModelMetaclass.__new__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\fields.py:411\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\fields.py:342\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.__init__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\fields.py:451\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField.prepare\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\fields.py:545\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydantic\\fields.py:550\u001B[0m, in \u001B[0;36mpydantic.fields.ModelField._type_analysis\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\typing.py:852\u001B[0m, in \u001B[0;36m_SpecialGenericAlias.__subclasscheck__\u001B[1;34m(self, cls)\u001B[0m\n\u001B[0;32m    850\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__origin__)\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mcls\u001B[39m, _GenericAlias):\n\u001B[1;32m--> 852\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43missubclass\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__origin__\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__subclasscheck__\u001B[39m(\u001B[38;5;28mcls\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "# NLP imports\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# general numerical and visualization imports\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65b0bd4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:28:42.609796300Z",
     "start_time": "2024-01-30T17:28:42.556424300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ritur\\AppData\\Local\\Temp\\ipykernel_19576\\3556721632.py:28: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "#displaying the most frequent 10 words\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')  # 'Agg' backend for saving plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'tokens' is your list of tokens\n",
    "word_freq = FreqDist(tokenized_text)\n",
    "\n",
    "# Get the 10 most common words\n",
    "most_common_words = word_freq.most_common(10)\n",
    "\n",
    "all_fdist = pd.Series(dict(most_common_words))\n",
    "# Setting fig and ax into variables\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "# Plot with Seaborn plotting tools\n",
    "plt.xticks(rotation = 60)\n",
    "plt.title(\"Frequency -- Top 10 Words in the input text file\",\n",
    "fontsize = 25)\n",
    "plt.xlabel(\"Words\", fontsize = 25)\n",
    "plt.ylabel(\"Frequency\", fontsize = 25)\n",
    "all_plot = sns.barplot(x = all_fdist.index, y = all_fdist.values,\n",
    "ax=ax)\n",
    "plt.xticks(rotation=50)\n",
    "#to display in UI\n",
    "plt.show()\n",
    "\n",
    "# Specify the path for the output image file where you want to save the plot\n",
    "#output_image_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q5_frequent_10_words_bar_plot.png\"\n",
    "\n",
    "# Save the plot as an image file\n",
    "#plt.savefig(output_image_path, bbox_inches='tight')  # 'bbox_inches' prevents trimming of labels\n",
    "\n",
    "#print(\"5. Displaying the most frequent 10 words saved as:\", output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9807c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T17:20:40.146736200Z"
    }
   },
   "outputs": [],
   "source": [
    "#6. Compute a word cloud from the word frequency distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076744c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T17:20:40.147760600Z"
    }
   },
   "outputs": [],
   "source": [
    "# displaying a WordCloud\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color = 'white',\n",
    "\n",
    "max_words = 25,\n",
    "relative_scaling = 0,\n",
    "width = 600,height = 300,\n",
    "max_font_size = 150,\n",
    "colormap = 'Dark2',\n",
    "min_font_size = 10).generate_from_frequencies(all_fdist)\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "#to display in UI\n",
    "#plt.show()\n",
    "# Specify the path for the output image file where you want to save the plot\n",
    "output_image_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q6_word_cloud_frequency.png\"\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig(output_image_path, bbox_inches='tight')  # 'bbox_inches' prevents trimming of labels\n",
    "\n",
    "print(\"6. Compute a word cloud from the word frequency distribution saved as:\", output_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb0f8c3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T17:20:40.149737500Z"
    }
   },
   "outputs": [],
   "source": [
    "#7. Display the frequencies of the parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3e606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T17:20:40.150701600Z",
     "start_time": "2024-01-30T17:20:40.150701600Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count the occurrences of each POS tag\n",
    "pos_freq = Counter(tag for word, tag in pos_tag_text)\n",
    "\n",
    "# Extract the tags and their frequencies\n",
    "tags = list(pos_freq.keys())\n",
    "frequencies = list(pos_freq.values())\n",
    "\n",
    "# Define a custom color palette using seaborn\n",
    "custom_palette = sns.color_palette(\"Set1\", len(tags))\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate through tags and frequencies and assign custom colors\n",
    "for i, (tag, freq) in enumerate(zip(tags, frequencies)):\n",
    "    plt.bar(tag, freq, color=custom_palette[i])\n",
    "\n",
    "plt.xlabel(\"Part of Speech\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Part of Speech Frequency in input text file\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()  # Ensure labels are not cut off\n",
    "#to show in the front end\n",
    "#plt.show()\n",
    "\n",
    "# Specify the path for the output image file where you want to save the plot\n",
    "output_image_path = \"E:\\\\MS_Course_Notes\\\\COMP_293C\\\\Assignments\\\\Assignment_1\\\\q7_frequency_pos.png\"\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig(output_image_path, bbox_inches='tight')  # 'bbox_inches' prevents trimming of labels\n",
    "\n",
    "print(\"7. Display the frequencies of the parts of speech. saved as:\", output_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680ce6d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T17:20:40.151694400Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
